{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb5e176-b0c5-42dc-bed7-a20943bc4eab",
   "metadata": {},
   "source": [
    "# DSCI 525 - Web and Cloud Computing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf2136e-ab6b-4954-804c-3f0b5c2ca226",
   "metadata": {},
   "source": [
    "### Milestone 1: Tackling big data on your laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1add9e4-5309-4ec4-97c2-22cf3996a82d",
   "metadata": {},
   "source": [
    "### Group 13: Kelly Wu, Julie Song, Bruce Wu, Vincent Ho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e2fab-ae27-49bf-9634-25b20f203b93",
   "metadata": {},
   "source": [
    "### 3. Downloading the data \n",
    "rubric={correctness:10}\n",
    "\n",
    "1. Download the data from [figshare](https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681) to your local computer using the [figshare API](https://docs.figshare.com) (you need to make use of `requests` library).\n",
    "\n",
    "2. Extract the zip file, again programmatically, similar to how we did it in class. \n",
    "\n",
    ">  You can download the data and unzip it manually. But we learned about APIs, so we can do it in a reproducible way with the `requests` library, similar to how we [did it in class](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture1.html#using-rest-api-lab-lecture).\n",
    "\n",
    "> There are 5 files in the figshare repo. The one we want is: `data.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd1b9147-41d8-46af-8c5b-05558e607a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507966c0-17e9-4dfa-8c09-5589631f0b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = 14096681"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "020f3a51-39dd-4c4f-be89-33f143153ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figsharerainfall/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541112c1-4622-41eb-aaac-1205afb38735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 26579150,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'size': 58863,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e'},\n",
       " {'id': 26579171,\n",
       "  'name': 'environment.yml',\n",
       "  'size': 192,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34'},\n",
       " {'id': 26586554,\n",
       "  'name': 'README.md',\n",
       "  'size': 5422,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c'},\n",
       " {'id': 26766812,\n",
       "  'name': 'data.zip',\n",
       "  'size': 814041183,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9'},\n",
       " {'id': 26766815,\n",
       "  'name': 'get_data.py',\n",
       "  'size': 4113,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  \n",
    "files = data[\"files\"]            \n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2283982b-5a70-46f2-9640-2408ca9fbb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.33 s, sys: 6.83 s, total: 11.2 s\n",
      "Wall time: 16min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"] \n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eddc807-3c3d-4737-a9f6-d4688184be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.7 s, sys: 1.1 s, total: 8.8 s\n",
      "Wall time: 8.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51acc2b-607a-45d5-a1aa-fdfc89bc1fa6",
   "metadata": {},
   "source": [
    "### 4. Combining data CSVs\n",
    "rubric={correctness:10,reasoning:10}\n",
    "\n",
    "1. Combine data CSVs into a single CSV using pandas.\n",
    "    \n",
    "2. When combining the CSV files, add an extra column called \"model\" that identifies the model.\n",
    "    Tip 1: you can get this column populated from the file name, eg: for file name \"SAM0-UNICON_daily_rainfall_NSW.csv\", the model name is SAM0-UNICON\n",
    "    Tip 2: Remember how we added \"year\" column when we combined airline CSVs. Here the regex will be to get word before an underscore ie, \"/([^_]*)\"\n",
    "\n",
    "> Note: There is a file called `observed_daily_rainfall_SYD.csv` in the data folder that you downloaded. Make sure you exclude this file (programmatically or just take out that file from the folder) before you combine CSVs. We will use this file in our next milestone.\n",
    "\n",
    "3. ***Compare*** run times on different machines within your team and summarize your observations. \n",
    "\n",
    "> Warning: Some of you might not be able to do it on your laptop. It's fine if you're unable to do it. Just make sure you discuss the reasons why you might not have been able to run this on your laptop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc6d498-d115-4947-bae8-bbc9a6d44ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observed_daily_rainfall_SYD.csv in the downloaded folder\n",
    "\n",
    "os.remove('figsharerainfall/observed_daily_rainfall_SYD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1177396-0e27-46c0-8f52-cd7b57191bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 20s, sys: 10.5 s, total: 3min 31s\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "files = glob.glob('figsharerainfall/*.csv')\n",
    "df = pd.concat((pd.read_csv(file, index_col=0)\n",
    "                .assign(model=re.findall(\"/([^_]*)\", file)[0])\n",
    "                for file in files)\n",
    "              )\n",
    "\n",
    "df.to_csv(\"figsharerainfall/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc3c0ed6-73d9-41e4-8266-fefd3f49a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\tfigsharerainfall/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh figsharerainfall/combined_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9e0958e-9063-4884-b5ca-1ebc20ef1ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62467843, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0efe6c0f-e1ac-41d1-940c-04179fed4468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1889-01-01 12:00:00</th>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.244226e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-02 12:00:00</th>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.217326e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-03 12:00:00</th>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.498125e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-04 12:00:00</th>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.251282e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-05 12:00:00</th>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.270161e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       lat_min    lat_max   lon_min   lon_max  rain (mm/day)  \\\n",
       "time                                                                           \n",
       "1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   4.244226e-13   \n",
       "1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   4.217326e-13   \n",
       "1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   4.498125e-13   \n",
       "1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   4.251282e-13   \n",
       "1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   4.270161e-13   \n",
       "\n",
       "                               model  \n",
       "time                                  \n",
       "1889-01-01 12:00:00  MPI-ESM-1-2-HAM  \n",
       "1889-01-02 12:00:00  MPI-ESM-1-2-HAM  \n",
       "1889-01-03 12:00:00  MPI-ESM-1-2-HAM  \n",
       "1889-01-04 12:00:00  MPI-ESM-1-2-HAM  \n",
       "1889-01-05 12:00:00  MPI-ESM-1-2-HAM  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eaa3f67-5ab6-48b5-ab91-12292d0bbb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Member</th>\n",
       "      <th>Operating System</th>\n",
       "      <th>RAM</th>\n",
       "      <th>Processor</th>\n",
       "      <th>Is SSD</th>\n",
       "      <th>Time taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julie</td>\n",
       "      <td>MacOS</td>\n",
       "      <td>16GB</td>\n",
       "      <td>Apple M1 Pro</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4min 33s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kelly</td>\n",
       "      <td>MacOS</td>\n",
       "      <td>16GB</td>\n",
       "      <td>2.8 GHz Quad-Core Intel Core i7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4min 50s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bruce</td>\n",
       "      <td>Windows11</td>\n",
       "      <td>16GB</td>\n",
       "      <td>12th Gen Intel(R) Core(TM) i7-12700H</td>\n",
       "      <td>Yes</td>\n",
       "      <td>12min 4s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vincent</td>\n",
       "      <td>MacOS</td>\n",
       "      <td>8GB</td>\n",
       "      <td>Apple M1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3min 33s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Team Member Operating System   RAM                             Processor  \\\n",
       "0       Julie            MacOS  16GB                          Apple M1 Pro   \n",
       "1       Kelly            MacOS  16GB       2.8 GHz Quad-Core Intel Core i7   \n",
       "2       Bruce        Windows11  16GB  12th Gen Intel(R) Core(TM) i7-12700H   \n",
       "3     Vincent            MacOS   8GB                              Apple M1   \n",
       "\n",
       "  Is SSD Time taken  \n",
       "0    Yes   4min 33s  \n",
       "1    Yes   4min 50s  \n",
       "2    Yes   12min 4s  \n",
       "3    Yes   3min 33s  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_diff_four = {'Team Member': ['Julie', 'Kelly', 'Bruce', 'Vincent'],\n",
    "        'Operating System': ['MacOS', 'MacOS', 'Windows11', 'MacOS'],\n",
    "        'RAM': ['16GB', '16GB', '16GB', '8GB'],\n",
    "        'Processor': ['Apple M1 Pro', '2.8 GHz Quad-Core Intel Core i7', '12th Gen Intel(R) Core(TM) i7-12700H', 'Apple M1'],\n",
    "        'Is SSD': ['Yes', 'Yes', 'Yes', 'Yes'],\n",
    "        'Time taken': ['4min 33s', '4min 50s', '12min 4s', '3min 33s']}\n",
    "\n",
    "pd.DataFrame(time_diff_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d4e3a-b634-4be3-90a0-b4dae5ec9aef",
   "metadata": {},
   "source": [
    "Among the processors, it appears that the task was generally performed more efficiently by MacOS's processors, as they took half the time to combine different CSV files into a single CSV with a size of 5.6GB compared to Windows' processors. While it seems that the Apple M1 chip performed more efficiently than the Intel chip among MacOS processors, the difference is not significant when comparing to MacOS versus Windows11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a75b1c-8368-4548-bdf8-0e8ea61bc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "## This is to clear the memory, and you get a fresh start. When you run this, you will lose all the variables that you have created so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6a942-f1b7-4e35-a401-6f09b81739aa",
   "metadata": {},
   "source": [
    "### 5. Load the combined CSV to memory and perform a simple EDA\n",
    "rubric={correctness:10,reasoning:10}\n",
    "\n",
    "1. Investigate at least two of the following approaches to reduce memory usage while performing the EDA (e.g., value_counts). \n",
    "    - Changing `dtype` of your data\n",
    "    - Load just columns that we want\n",
    "    - Loading in chunks\n",
    "    \n",
    "2. ***Compare*** run times on different machines within your team and summarize your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e4e808-60b3-4672-89fc-b2978d6afde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e549cc0-4843-4e37-9eb1-4e9237ee0d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "NorESM2-MM          3541230\n",
      "TaiESM1             3541230\n",
      "SAM0-UNICON         3541153\n",
      "GFDL-ESM4           3219300\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "MRI-ESM2-0          3037320\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "FGOALS-g3           1287720\n",
      "KIOST-ESM           1287720\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "Name: model, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62467843 entries, 0 to 62467842\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   time           object \n",
      " 1   rain (mm/day)  float32\n",
      " 2   model          object \n",
      "dtypes: float32(1), object(2)\n",
      "memory usage: 1.2+ GB\n",
      "None\n",
      "       rain (mm/day)\n",
      "count   5.924854e+07\n",
      "mean    1.901173e+00\n",
      "std     5.585735e+00\n",
      "min    -3.807373e-12\n",
      "25%     3.838413e-06\n",
      "50%     6.154947e-02\n",
      "75%     1.020918e+00\n",
      "max     4.329395e+02\n",
      "CPU times: user 27.9 s, sys: 5 s, total: 32.9 s\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "# Load three columns only: \"time\", \"rain (mm/day)\", \"model\"\n",
    "# Changing dtype of 'rain (mm/day)' from float64 to float32\n",
    "\n",
    "%%time\n",
    "use_cols = [\"time\", \"rain (mm/day)\", 'model']\n",
    "df_only3col = pd.read_csv(\"figsharerainfall/combined_data.csv\",usecols=use_cols, \n",
    "                 dtype = {'rain (mm/day)': np.float32})\n",
    "print(df_only3col[\"model\"].value_counts())\n",
    "print(df_only3col.info())\n",
    "print(df_only3col.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ef92411-652f-4841-bf19-12b72acbe141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Member</th>\n",
       "      <th>Operating System</th>\n",
       "      <th>RAM</th>\n",
       "      <th>Processor</th>\n",
       "      <th>Is SSD</th>\n",
       "      <th>Time taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julie</td>\n",
       "      <td>MacOS</td>\n",
       "      <td>16GB</td>\n",
       "      <td>Apple M1 Pro</td>\n",
       "      <td>Yes</td>\n",
       "      <td>47.2 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kelly</td>\n",
       "      <td>MacOS</td>\n",
       "      <td>16GB</td>\n",
       "      <td>2.8 GHz Quad-Core Intel Core i7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>59.6 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bruce</td>\n",
       "      <td>Windows11</td>\n",
       "      <td>16GB</td>\n",
       "      <td>12th Gen Intel(R) Core(TM) i7-12700H</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1min 16s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vincent</td>\n",
       "      <td>MacOS</td>\n",
       "      <td>8GB</td>\n",
       "      <td>Apple M1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>35.3 s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Team Member Operating System   RAM                             Processor  \\\n",
       "0       Julie            MacOS  16GB                          Apple M1 Pro   \n",
       "1       Kelly            MacOS  16GB       2.8 GHz Quad-Core Intel Core i7   \n",
       "2       Bruce        Windows11  16GB  12th Gen Intel(R) Core(TM) i7-12700H   \n",
       "3     Vincent            MacOS   8GB                              Apple M1   \n",
       "\n",
       "  Is SSD Time taken  \n",
       "0    Yes     47.2 s  \n",
       "1    Yes     59.6 s  \n",
       "2    Yes   1min 16s  \n",
       "3    Yes     35.3 s  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_diff_five = {'Team Member': ['Julie', 'Kelly', 'Bruce', 'Vincent'],\n",
    "        'Operating System': ['MacOS', 'MacOS', 'Windows11', 'MacOS'],\n",
    "        'RAM': ['16GB', '16GB', '16GB', '8GB'],\n",
    "        'Processor': ['Apple M1 Pro', '2.8 GHz Quad-Core Intel Core i7', '12th Gen Intel(R) Core(TM) i7-12700H', 'Apple M1'],\n",
    "        'Is SSD': ['Yes', 'Yes', 'Yes', 'Yes'],\n",
    "        'Time taken': ['47.2 s', '59.6 s', '1min 16s', '35.3 s']}\n",
    "\n",
    "pd.DataFrame(time_diff_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e17aec-e23d-4104-9626-140effa84e6d",
   "metadata": {},
   "source": [
    "Among the processors, it appears that the task was generally performed more efficiently by MacOS's processors, as they took less time to load the combined CSV with 5.6GB into memory and perform a simple EDA. While it seems that MacOS's processors performed more efficiently than Windows' processors, the difference is not significant when comparing the results of combining the CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a653de-036d-4cac-b96e-ce40e021af2f",
   "metadata": {},
   "source": [
    "### 6. Perform a simple EDA in R\n",
    "rubric={correctness:15,reasoning:10}\n",
    "\n",
    "1. Choose one of the methods listed below for transferring the dataframe (i.e., the entire dataset) from Python to R, and explain why you opted for this approach instead of the others.\n",
    "    - [Parquet file](http://parquet.apache.org)\n",
    "    - [Pandas exchange](https://rpy2.github.io/doc/latest/html/interactive.html)\n",
    "    - [Arrow exchange](https://github.com/rpy2/rpy2-arrow)\n",
    "2. Once you have the dataframe in R, perform a simple EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f31ef86-de43-4cec-b7fe-64767cbb74b0",
   "metadata": {},
   "source": [
    "### Reasoning:\n",
    "\n",
    "We will choose Arrow exchange since Arrow is optimized for performance and can take advantage of multi-core processors and distributed systems to speed up data transfer. One of the key benefits of Arrow is that it supports zero-copy data sharing between different programming languages. This means that the data can be transferred between Python and R without the need for copying or converting the data, which can be time-consuming and memory-intensive. Besides, Arrow has good support for both Python and R, with libraries available for both languages, which means that it's easy to integrate Arrow into existing Python and R workflows.\n",
    "One disadvantage of using Pandas exchange is that it can be memory-intensive, especially for large datasets. When transferring data between Python and R using Pandas exchange, the entire dataset must be loaded into memory on both sides, which is not available for our dataset with 6GB. Moreover, Pandas exchange is not as efficient as Arrow exchange since it relies on serializing and deserializing data, which can be slower and less efficient than zero-copy data sharing.\n",
    "The disadvantage of Parquet file in our dataset could be the low efficiency of using Parquet file for our dataset as it spends much more time than the Arrow exchange. Moreover, the Arrow exchange uses a flat memory layout, which means that the memory required to store the data is minimized, whereas Parquet file has a more complex memory layout that requires more memory to store the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600e3cf0-4bcc-4158-847f-59ea9faba434",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715caab1-2083-4411-ad1d-78bb37728a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepathcsv = \"/Users/vincentho/Desktop/DSCI525_group13/notebooks/figsharerainfall/combined_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027ae7f8-482b-4e0f-be5a-dff1c084a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['R_HOME'] = '/Users/vincentho/opt/miniconda3/envs/525_2023/lib/R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db94c615-4999-4951-bc4b-abbf2d444fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b7c033-f4d6-4fc7-8cbf-9495cd360bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import pyarrow \n",
    "from pyarrow import csv\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82320a8f-8a0a-487b-896b-9b7a04e15534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 s, sys: 1.3 s, total: 13.5 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = ds.dataset(filepathcsv, format=\"csv\")\n",
    "\n",
    "table = dataset.to_table()\n",
    "\n",
    "r_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "278ae375-80d4-4968-ae49-3774c19d7816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 62,467,843 × 7\n",
      "   time                lat_min lat_max lon_min lon_max `rain (mm/day)` model    \n",
      "   <dttm>                <dbl>   <dbl>   <dbl>   <dbl>           <dbl> <chr>    \n",
      " 1 1889-01-01 04:00:00   -35.4   -33.6    142.    143.        4.24e-13 MPI-ESM-…\n",
      " 2 1889-01-02 04:00:00   -35.4   -33.6    142.    143.        4.22e-13 MPI-ESM-…\n",
      " 3 1889-01-03 04:00:00   -35.4   -33.6    142.    143.        4.50e-13 MPI-ESM-…\n",
      " 4 1889-01-04 04:00:00   -35.4   -33.6    142.    143.        4.25e-13 MPI-ESM-…\n",
      " 5 1889-01-05 04:00:00   -35.4   -33.6    142.    143.        4.27e-13 MPI-ESM-…\n",
      " 6 1889-01-06 04:00:00   -35.4   -33.6    142.    143.        4.20e-13 MPI-ESM-…\n",
      " 7 1889-01-07 04:00:00   -35.4   -33.6    142.    143.        4.19e-13 MPI-ESM-…\n",
      " 8 1889-01-08 04:00:00   -35.4   -33.6    142.    143.        4.56e-13 MPI-ESM-…\n",
      " 9 1889-01-09 04:00:00   -35.4   -33.6    142.    143.        2.53e+ 0 MPI-ESM-…\n",
      "10 1889-01-10 04:00:00   -35.4   -33.6    142.    143.        4.12e- 2 MPI-ESM-…\n",
      "# ℹ 62,467,833 more rows\n",
      "# ℹ Use `print(n = ...)` to see more rows\n",
      "Time difference of 0.0002851486 secs\n",
      "CPU times: user 2.37 s, sys: 1.69 s, total: 4.06 s\n",
      "Wall time: 4.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R -i r_table\n",
    "start_time <- Sys.time()\n",
    "suppressMessages(library(dplyr))\n",
    "\n",
    "\n",
    "result <- r_table \n",
    "\n",
    "end_time <- Sys.time()\n",
    "\n",
    "print(result %>% collect())\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ecde2f6b-0f18-4a52-976d-ddb2be9164d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 27 × 3\n",
      "   model           count average_rainfall\n",
      "   <chr>           <int>            <dbl>\n",
      " 1 INM-CM4-8     1609650             2.81\n",
      " 2 INM-CM5-0     1609650             2.67\n",
      " 3 CMCC-CM2-SR5  3541230             2.38\n",
      " 4 MIROC6        2070900             2.30\n",
      " 5 CMCC-CM2-HR4  3541230             2.28\n",
      " 6 CMCC-ESM2     3541230             2.27\n",
      " 7 NorESM2-MM    3541230             2.23\n",
      " 8 NorESM2-LM     919800             2.23\n",
      " 9 TaiESM1       3541230             2.22\n",
      "10 ACCESS-ESM1-5 1610700             2.22\n",
      "# ℹ 17 more rows\n",
      "# ℹ Use `print(n = ...)` to see more rows\n",
      "Time difference of 0.04090595 secs\n",
      "CPU times: user 1.6 s, sys: 1.06 s, total: 2.66 s\n",
      "Wall time: 732 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R -i r_table\n",
    "start_time <- Sys.time()\n",
    "suppressMessages(library(dplyr))\n",
    "\n",
    "\n",
    "result <- r_table %>% group_by(model) %>% summarize(count=n(), average_rainfall = mean(`rain (mm/day)`, na.rm = TRUE)) %>% arrange(desc(average_rainfall))\n",
    "\n",
    "end_time <- Sys.time()\n",
    "\n",
    "print(result %>% collect())\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a17ce3-d129-4c40-b70c-061b2b336992",
   "metadata": {},
   "source": [
    "## Challenges or Difficulties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228ca8c-d859-49c0-bd31-fdbb528826a6",
   "metadata": {},
   "source": [
    "One of the challenges of dealing with big data is that we have to always pay attention to the amount of storage space required to store the data. It is important as repeating steps or code chunks such as combining a new 5.6GB CSV file could quickly fill up storage space and cause the laptop to crash, especially if it has limited storage capacity.\n",
    "Another challenge of working with big data is the time-consuming nature of the process. It can take a considerable amount of time to realize that the code at the bottom of a program is not working properly, and then we will need to spend a significant amount of additional time rerunning the entire process again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525_2023]",
   "language": "python",
   "name": "conda-env-525_2023-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
